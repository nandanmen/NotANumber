---
title: Tokenizer
---

As mentioned in [[A Quick Rundown]], Babel has four distinct phases in its pipeline: tokenization, parsing, transformation, and code generation. The **tokenizer** is the piece of code responsible for handling the first phase - the tokenization phase.

A tokenizer takes your raw code string as input and generates a list of _tokens_ as its output. A _token_ can be thought of as a JavaScript "word" - the smallest sequence of characters that still carry a meaning in the language. For example, the following JS code:

```js
console.log(message)
```

Corresponds to the following tokens:

```
[console] [.] [log] [(] [message] [)]
```

Each token has a type that describes what that sequence of characters represent. In the example above, the respective token types are as follows:

```
[console]   [.]  [log]       [(]        [message]   [)]
 Identifier  Dot  Identifier  LeftParen  Identifier  RightParen
```

So how does the tokenizer generate these tokens?

## First Attempt

In my first attempt with the tokenizer, I aimed to simply split the string into the tokens - I didn't want to overcomplicate by introducing a Token class or anything.

The boilerplate was pretty simple - have a cursor that points to the currently existing token, and iterate through all the tokens using that cursor:

```ts
function tokenize(code: string): string[] {
  let current = 0
  let tokens = []

  while (current < code.length) {
    // parse tokens
    current++
  }

  return tokens
}
```

### Single Character Tokens

I started with the simplest tokens first, the single character tokens:

```ts
const singleCharacterTokens = new Set(['(', ')', '{', '}', ';', '.'])
```

As we iterate through the code string, we check if the current character points to one of these single character tokens. If it does, we add the character to the final tokens list. We'll discard all other characters for now.

_Instead of showing the code itself, it would be cool to have the reader implement the this themselves using a visualization as a guide._

```ts
const singleCharacterTokens = new Set(['(', ')', '{', '}', ';', '.'])

function tokenize(code: string): string[] {
  let current = 0
  let tokens = []

  while (current < code.length) {
    const char = code[current]
    if (singleCharacterTokens.has(char)) {
      tokens.push(char)
    }
    current++
  }

  return tokens
}
```

```
// visual here
```

### Identifiers and Keywords

An identifier in JavaScript is a sequence of characters that is used to refer to a variable name, like `message` and `hello`. Some sequences are called _keywords_ because they have special meaning in the language itself and _cannot_ be used as a variable name - some examples include `function`, `while`, and `switch`.

Identifiers and keywords are a bit more complicated to tokenize because they can be more than one character long. In JavaScript, a valid identifier is a sequence of alphanumeric characters and underscores, except the first character cannot be a number.

This means the following strings are valid identifiers:

```
hello
_abc
abc123
```

But the following are not:

```
2cool
8ball
```

I wanted to keep it simple, so in my first pass, I only supported identifiers that are _exclusively_ alphabetical. The idea that I had was this:

1. If the current character is alphabetical, start parsing an identifier;
2. We keep adding characters to the identifier' until the current character _isn't_ alphabetical.

```
// visual here
```

```ts
const singleCharacterTokens = new Set(['(', ')', '{', '}', ';', '.'])

function tokenize(code: string): string[] {
  let start = 0
  let current = 0
  let tokens = []

  function finishIdentifier() {
    while (isAlpha(code[current])) {
      current++
    }
    const name = code.substring(start, current)
    start = current
    return name
  }

  while (current < code.length) {
    const char = code[current]
    if (isAlpha(char)) {
      tokens.push(finishIdentifier())
    } else if (singleCharacterTokens.has(char)) {
      tokens.push(char)
      start++
      current++
    }
  }

  return tokens
}
```

### Whitespace

Finally, the last thing we need to take care of is _whitespace_. We don't really want to do anything with whitespace, so we will simply skip the current character if it's whitespace:

```ts
const singleCharacterTokens = new Set(['(', ')', '{', '}', ';', '.'])

function isAlpha(char: string) {
  return /[a-zA-Z]/.test(char)
}

function isWhitespace(char: string) {
  return /\s/.test(char)
}

function tokenize(code: string): string[] {
  let start = 0
  let current = 0
  let tokens = []

  function finishIdentifier() {
    while (isAlpha(code[current])) {
      current++
    }
    const name = code.substring(start, current)
    start = current
    return name
  }

  while (current < code.length) {
    const char = code[current]
    if (isAlpha(char)) {
      tokens.push(finishIdentifier())
    } else if (singleCharacterTokens.has(char)) {
      tokens.push(char)
      start++
      current++
    } else if (isWhitespace(char)) {
      start++
      current++
    }
  }

  return tokens
}
```

And that's it! We're now able to parse enough tokens to recognize all of the tokens of our input:

```js
function hello(message) {
  console.log(message)
}
```

## Refactoring

Our code has one issue though: we're only returning the tokens as literal strings! This isn't much more useful for the rest of the compiler pipeline than the initial raw code is. It would be a lot nicer if we return some information about the token's _type_ as well. This would also let us differentiate between _identifiers_ and _keywords_, which we haven't done so far.

Let's start by creating a list of all the token types:

```ts
enum TokenType {
  Keyword = 'Keyword',
  Identifier = 'Identifier',
  LeftParen = 'LeftParen',
  RightParen = 'RightParen',
  LeftCurly = 'LeftCurly',
  RightCurly = 'RightCurly',
  Dot = 'Dot',
  Semicolon = 'Semicolon',
}
```

_I've purposely assigned each enum value to a string here so that it's easier to debug; if we didn't do this, logging the token type would only show a number like 0 or 1._

The token object that we're going to be using consists of a `type` property and an optional `name` property for keywords and identifiers:

```ts
type Token = {
  type: TokenType
  name?: string
}
```

To make it easier to create these objects, let's make builder functions for each token type:

```ts
const token = {
  keyword(name: string) {
    return {
      type: TokenType.Keyword,
      name,
    }
  },
  identifier(name: string) {
    return {
      type: TokenType.Identifier,
      name,
    }
  },
  leftParen() {
    return { type: TokenType.LeftParen }
  },
  rightParen() {
    return { type: TokenType.RightParen }
  },
  leftCurly() {
    return { type: TokenType.LeftCurly }
  },
  rightCurly() {
    return { type: TokenType.RightCurly }
  },
  dot() {
    return { type: TokenType.Dot }
  },
  semicolon() {
    return { type: TokenType.Semicolon }
  },
}
```

This lets us construct a token by simply calling `token.<type>()`, e.g. `token.keyword('function')` to create a token that corresponds to the `function` keyword.

Great! All that's left is to update our `tokenize` function to use these builder functions instead of returning the string literal. First, let's update the `finishIdentifier()` function to create an identifier or a keyword:

```ts
const keywords = new Set(['function'])

function tokenize(code: string): string[] {
  let start = 0
  let current = 0
  let tokens = []

  function finishIdentifier() {
    while (isAlpha(code[current])) {
      current++
    }
    const name = code.substring(start, current)
    start = current

    if (keywords.has(name)) {
      return token.keyword(name)
    }

    return token.identifier(name)
  }

  while (current < code.length) {
    const char = code[current]
    if (isAlpha(char)) {
      tokens.push(finishIdentifier())
    } else if (singleCharacterTokens.has(char)) {
      tokens.push(char)
      start++
      current++
    } else if (isWhitespace(char)) {
      start++
      current++
    }
  }

  return tokens
}
```

Notice that I also added a set to contain the list of known keywords. For now, since we're only parsing the hello function, the only known keyword is "function".

For the single character tokens, we need a way to map each character string to the appropriate builder function. For example, if we see the string `";"` then we need to call the `token.semicolon()` function.

To do this, I created a `Map` that defines this relationship:

```ts
type SingleCharacterToken = '(' | ')' | '{' | '}' | '.' | ';'

const knownSingleCharacters = new Map<SingleCharacterToken, () => Token>([
  ['(', token.leftParen],
  [')', token.rightParen],
  ['{', token.leftCurly],
  ['}', token.rightCurly],
  ['.', token.dot],
  [';', token.semicolon],
])
```

And added a type guard to determine if the current character is a single character token:

```ts
function isSingleCharacter(char: string): char is SingleCharacterToken {
  return knownSingleCharacters.has(char as SingleCharacterToken)
}
```

Then I added another helper that returns the appropriate builder function for a single character token:

```ts
function getCharToken(char: SingleCharacterToken) {
  const builder = knownSingleCharacters.get(char)
  // we need the exclamation mark here because TS would say
  // that this is undefined otherwise.
  return builder!()
}
```

Now we can update the `tokenize` function to call these helpers:

```ts
function tokenize(input: string): Token[] {
  let start = 0
  let current = 0
  const tokens = []

  function finishIdentifier() {
    while (isAlpha(input[current])) {
      current++
    }
    const name = input.substring(start, current)
    start = current

    if (keywords.has(name)) {
      return token.keyword(name)
    }

    return token.identifier(name)
  }

  while (current < input.length) {
    const currentChar = input[current]

    if (isWhitespace(currentChar)) {
      start++
      current++
      continue
    }

    if (isAlpha(currentChar)) {
      tokens.push(finishIdentifier())
    } else if (isSingleCharacter(currentChar)) {
      tokens.push(getCharToken(currentChar))
      start++
      current++
    }
  }

  return tokens
}
```

And as a finishing touch, let's throw an error if we find a character we can't parse just yet:

```ts
function tokenize(input: string): Token[] {
  let start = 0
  let current = 0
  const tokens = []

  function finishIdentifier() {
    while (isAlpha(input[current])) {
      current++
    }
    const name = input.substring(start, current)
    start = current

    if (keywords.has(name)) {
      return token.keyword(name)
    }

    return token.identifier(name)
  }

  while (current < input.length) {
    const currentChar = input[current]

    if (isWhitespace(currentChar)) {
      start++
      current++
      continue
    }

    if (isAlpha(currentChar)) {
      tokens.push(finishIdentifier())
    } else if (isSingleCharacter(currentChar)) {
      tokens.push(getCharToken(currentChar))
      start++
      current++
    } else {
      throw new Error(`Unknown character: ${currentChar}`)
    }
  }

  return tokens
}
```

Now if we run this function, we get exactly the output we would expect!

```
exec tokenizer
```

## Summary

And there we have our tokenizer! It can't do too much at the moment, but it's able to tokenize the input that we want it to - this simple hello function:

```js
function hello(message) {
  console.log(message)
}
```

It might also seem a tad over-engineered, but I think it's general enough that we can extend it later on when we implement more features of the JS language.

In the next post, we'll take the tokens generated in this phase and try to parse them into a syntax tree. Until then!
