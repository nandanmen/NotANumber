---
title: "Build Your Own Database"
blurb: "How would you reinvent the database?"
description: "How would you reinvent the database?"
publishedAt: "2022-11-15"
editedAt: "2022-11-22"
---

import { Callout } from "~/components/Callout";
import { ProblemStatement } from "~/components/ProblemStatement";
import { AppendOnlyFile } from "./components/AppendOnlyFile";
import { FileDatabaseExample } from "./components/FileDatabaseExample";
import { SingleFileCompaction } from "./components/SingleFileCompaction";
import { Segments } from "./components/Segments";
import { SegmentCompaction } from "./components/SegmentCompaction";

If you were to build your own database today, not knowing that databases exist already, how would you do it? In this post, we'll explore how to build a **key-value database** from the ground up.

A key-value database works more or less like objects in JavaScript—you can store values using a key and retrieve them later using that same key:

```sh
$ db set 'hello' 'world'
$ db get 'hello'
world
```

Let's get started!

## The Humble File

Databases were made to solve a simple problem: how do you store data persistently and efficiently look it up later?

Let's start by using a file. When we want to store data, we add the key-value pair to the file; when we want to look for a specific key, we iterate through the pairs to see if there's a matching key:

<AppendOnlyFile mode={["add", "search"]} />

What about updates and deletes?

For starters, we could go through the file, find the key, and either replace the value or delete the pair entirely. But it turns out we don't really want to do that—hard drives are (generally speaking) much better at _appending_ to a file than they are at _modifying_ a file.

<Callout label="Why is appending to a file faster?">

A disk in a hard drive is divided into _tracks_, with each track divided into _sectors_. Each sector is a fixed size, and the hard drive can only read or write data in whole sectors using a _disk head_.

When the hard disk reads or writes data, it needs to move the disk head to the specific sector. Moving the disk head is much slower than simply reading or writing data, so we want to minimize the number of times the disk head is moved.

When we append to a file, we're always adding data to the end of the file, so we only need to move the disk head once. When we modify a file, we potentially have to move the disk head multiple times, since we might need to read or write data from different sectors. This makes modifying a file much slower!

</Callout>

Since we're limiting ourselves to only appending to a file, we have to get a bit creative with updating and deleting pairs. One way to do updates in an append-only file is to treat the update like you're inserting a new key—add it to the end of the file:

<AppendOnlyFile mode={["add", "update"]} />

But now we have another problem—there are duplicate keys in the file! To work around this, we have to change our search algorithm to look for the _last_ occurrence of the key instead of the first:

```
search with duplicates example
```

To delete records, we create a special "tombstone" record that marks the key as deleted. There's no single way to do this, but one way is to use a special value like `null`:

<AppendOnlyFile mode={["add", "delete"]} />

And there we have it! We have a key-value database that uses a file as its storage mechanism. Using it, we can store, find, update, and delete key-value pairs.

### File Troubles

This implementation is nowhere near perfect, however; in fact, there are two major issues:

1. **The file can get very large**. Since we're only appending to the file, the file will grow infinitely over time. This is not good!
2. **Searching is slow**. To search for a specific key, we have to potentially iterate through all records in the database. For a database with millions of records, this can take a while!

How can we fix these problems?

## Keeping Files Small

Let's start with the first:

<ProblemStatement>

**How do we make sure the file doesn't grow indefinitely?** Because we're using an append-only file, we need some mechanism to periodically "shrink" the file so it doesn't eventually take over our entire hard drive.

</ProblemStatement>

Take a look at our database here after a few updates and deletes:

<FileDatabaseExample
  records={[
    {
      value: [1, "Lorem ipsum"],
    },
    {
      value: [12, "adipiscing elit."],
    },
    {
      value: [1, "null"],
      type: "active",
    },
    {
      value: [18, "dolor sit"],
    },
    {
      value: [12, "Vestibulum varius"],
      type: "success",
    },
    {
      value: [18, "vel mauris"],
      type: "success",
    },
  ]}
/>

Notice that our database file contains six records, but only two are active—the rest are either deleted or contain stale data. If we can clear all the irrelevant data, we can shrink the file by over 66%!

### First Attempt at Compaction

One thing we can do is to periodically copy over all the "active" records to a new file:

<SingleFileCompaction />

Notice that because we're working on a new file, the existing one can still accept new records, like the keys `014` and `006` in the above animation.

Wait a second. Those new records were added to the old file but weren't copied to the new one! What should we do? We _could_ rerun the compaction process, but what if new records come in during _that_ process?

Hmm, let's take a step back. **Maybe there's another way to organize the data?**

### Splitting to Segments

Both of these files contain up-to-date values for keys `12` and `18`, so what if we delete the values in one of the files?

Of course, deleting these records isn't as straightforward as it seems—remember that the files are append-only, so we can't modify records that were already written. But **what if they were never part of the same file, to begin with?**

Here's what we'll do. When our file exceeds a specific size, we'll **create a new file** and start writing to that one instead. We'll call these files _segments_:

<Segments />

We'll then run our compaction process on the _old_ segments, potentially merging them along the way:

<SegmentCompaction />

"Old" is the keyword here—because we're not merging data from the current "active" segment, that segment can freely accept new records (as shown in the bottom-most segment above).

Once the compaction is complete, we can safely delete the old segments:

```
redirect
```

### Adapting to Segments

Now that we're using segments, we need to change our search algorithm to start looking in the most **recent** segment first, working backward:

```
segment search example
```

## Your First Index

That's one problem solved; on to the next:

<ProblemStatement>

**How do we make searching fast?** Right now, we have to iterate through all of the records in the database to find a specific key. This is slow!

</ProblemStatement>

What if we use _objects_? That's right, these little guys:

```js
const hashTable = {};
```

In other languages, JavaScript objects are called _hash tables_ or _dictionaries_ or _maps_. Hash tables are great because they're really good at storing key-value pairs and quickly looking them up later. That sounds exactly like what we need!

Here's how it'll work. For every key that we have in our database, we'll store the record's **offset** in the index:

```
offset example
```

I purposely showed the file a little differently this time. Here, I've portrayed the file more like what the computer might see — a series of contiguous characters. This makes it easier to show each record's offsets.

<Callout label="What are offsets?">

A record's offset is **the number of bytes from the beginning of the file to the start of the record**. For example, the second record, `12: Vestibulum varius`, has an offset of 15 because:

1. Each character is 1 byte large;
2. The first record is 13 characters long (`1:Lorem ipsum`);
3. The first record ends with a newline character, which is 1 byte long;

This gives us an offset of `13 + 1 + 1 = 15`.

</Callout>

Notice that each segment has its own index because the offsets are relative to the start of each segment.

### Searching With Indices

Great! Now that we have our indices set up, our search algorithm now works a little differently:

1. Starting at the most recent segment, look up the key in the index;
2. If the key is found, read the record at the offset;
3. If the key is not found, repeat with the second most recent segment, and so on.

```
searching with index
```

### Updating Indices

Of course, we'll need to keep our indices up to date with the data — otherwise, they won't be useful! Specifically, whenever we update, delete, or insert a record, we have to change the index accordingly:

```
updating index
```

Notice what this implies — **writing to the database is slower with an index!** This is one of the tradeoffs of using an index; we can search for data much faster at the cost of slower writes.

### Tradeoffs

An index is great because it lets us query our database much faster, but there are some problems with our specific hash table implementation:

1. **Keys have to fit in memory**. Since we're using an in-memory hash table as our index, all of the keys in our database must fit in memory. This means there's a limit on the number of keys we can store!
2. **Range queries are still inefficient**. Our data isn't sorted in any way, so we can't do range queries efficiently. For example, if we wanted to find all the records between the keys `12` and `18`, our index wouldn't help—we'd have to iterate through the entire database!

## Sorted String Tables

Here's an idea: what if we **ensure our database is always sorted by key?** By sorting our data, we'll be able to solve both our problems at once. Let me explain.

### Faster Range Queries

If you have an unsorted array and you want to find all the elements between two specific values, you don't have much choice but to iterate through the whole array:

```
[5,3,2,4,1]
```

If your array is sorted, however, you'll only need to look for the first element and then iterate until you have all the elements you need:

```
[1,2,3,4,5]
```

- SSTs are an improvement because they allow for sparse indices and better range query performance
  - Though sparse, keys still have to fit in memory
  - How do you make sure writes are still fast and sequential? By using an LSM-Tree, i.e., add records to an in-memory balanced tree and then flush to disk in the SST format

## Persisting Trees on Disk

- How might you take a BST and persist it on disk?
  - It kinda feels like B-Trees are a lower level of abstraction because we're working directly with blocks here
  - I'm personally having a difficult time picturing disk pointers—are they just byte offsets?
- This requires a detour to how hard drives work imo

### How do Hard Drives Work?
