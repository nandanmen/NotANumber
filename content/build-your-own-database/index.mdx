---
title: "Build Your Own Database"
blurb: "How would you reinvent the database?"
description: "How would you reinvent the database?"
publishedAt: "2022-11-15"
editedAt: "2022-11-22"
---

import { Callout } from "~/components/Callout";
import { ProblemStatement } from "~/components/ProblemStatement";
import { AppendOnlyFile } from "./components/AppendOnlyFile";
import { FileDatabaseExample } from "./components/FileDatabaseExample";

If you were to build your own database today, not knowing that databases exist already, how would you do it? In this post, we'll explore how to build a **key-value database** from the ground up.

A key-value database works more or less like objects in JavaScript—you can store values using a key and retrieve them later using that same key:

```sh
$ db set 'hello' 'world'
$ db get 'hello'
world
```

Let's get started!

## The Humble File

Databases were made to solve a simple problem: how do you store data persistently and efficiently look it up later?

Let's start by using a file. When we want to store data, we add the key-value pair to the file; when we want to look for a specific key, we iterate through the pairs to see if there's a matching key:

<AppendOnlyFile mode={["add", "search"]} />

What about updates and deletes?

For starters, we could go through the file, find the key, and either replace the value or delete the pair entirely. But it turns out we don't really want to do that—hard drives are (generally speaking) much better at _appending_ to a file than they are at _modifying_ a file.

<Callout label="Why is appending to a file faster?">

A disk in a hard drive is divided into _tracks_, with each track divided into _sectors_. Each sector is a fixed size, and the hard drive can only read or write data in whole sectors using a _disk head_.

When the hard disk reads or writes data, it needs to move the disk head to the specific sector. Moving the disk head is much slower than simply reading or writing data, so we want to minimize the number of times the disk head is moved.

When we append to a file, we're always adding data to the end of the file, so we only need to move the disk head once. When we modify a file, we potentially have to move the disk head multiple times, since we might need to read or write data from different sectors. This makes modifying a file much slower!

</Callout>

Since we're limiting ourselves to only appending to a file, we have to get a bit creative with updating and deleting pairs. One way to do updates in an append-only file is to treat the update like you're inserting a new key—add it to the end of the file:

<AppendOnlyFile mode={["add", "update"]} />

But now we have another problem—there are duplicate keys in the file! To work around this, we have to change our search algorithm to look for the _last_ occurrence of the key instead of the first:

```
search with duplicates example
```

To delete records, we create a special "tombstone" record that marks the key as deleted. There's no single way to do this, but one way is to use a special value like `null`:

<AppendOnlyFile mode={["add", "delete"]} />

And there we have it! We have a key-value database that uses a file as its storage mechanism. Using it, we can store, find, update, and delete key-value pairs.

### File Troubles

This implementation is nowhere near perfect, however; in fact, there are two major issues:

1. **The file can get very large**. Since we're only appending to the file, the file will grow infinitely over time. This is not good!
2. **Searching is slow**. To search for a specific key, we have to potentially iterate through all records in the database. For a database with millions of records, this can take a while!

How can we fix these problems?

## Keeping Files Small

Let's start with the first:

<ProblemStatement>

**How do we make sure the file doesn't grow indefinitely?** Because we're using an append-only file, we need some mechanism to periodically "shrink" the file so it doesn't eventually take over our entire hard drive.

</ProblemStatement>

Take a look at our database here after a few updates and deletes:

<FileDatabaseExample
  records={[
    {
      value: [1, "Lorem ipsum"],
    },
    {
      value: [12, "adipiscing elit."],
    },
    {
      value: [1, "null"],
      type: "active",
    },
    {
      value: [18, "dolor sit"],
    },
    {
      value: [12, "Vestibulum varius"],
      type: "success",
    },
    {
      value: [18, "vel mauris"],
      type: "success",
    },
  ]}
/>

Notice that our database file contains six records, but only two are active—the rest are either deleted or contain stale data. If we can clear all the irrelevant data, we can shrink the file by over 66%!

### First Attempt at Compaction

One thing we can do is to periodically copy over all the active records to a new file. We can do this in the background so the main file can still accept new updates:

```
compaction example
```

What happens when we're done? We might be tempted to delete the old file, but we can't actually do that because it might contain new records that weren't copied over:

```
compaction deletion problem
```

So, after all that, we somehow ended up using _more_ disk space!

### Splitting to Segments

Take a look at these two files—do you notice any similarities?

```
file comparison
```

Both of these files contain up-to-date values for keys 12 and 18, so what if we delete the values in one of the files?

Of course, deleting these records isn't as straightforward as it seems—remember that the files are append-only, so we can't modify records that were already written. But **what if they were never part of the same file, to begin with?**

Here's what we'll do. When our file exceeds a specific size, we'll **create a new file** and start writing to that one instead. We'll call these files _segments_:

```
segments intro
```

We'll then run our compaction process on the _old_ segments, potentially merging them along the way:

```
segment compaction
```

"Old" is the keyword here—because we're not merging data from the current "active" segment, we don't have to worry about modifying that segment when we're done.

Note that we won't modify the segments as we're compacting them. This immutability lets us continue reading from the old segments while the compaction process is happening.

When the compaction is complete, we can redirect our reads to the compacted segments and safely delete the old segments:

```
redirect
```

## Speeding Up Searches

- Introduce in-memory indices with hash tables
- Main issues:
  - Keys have to fit in memory
  - Inefficient range queries

## Sorted String Tables

- SSTs are an improvement because they allow for sparse indices and better range query performance
  - Though sparse, keys still have to fit in memory
  - How do you make sure writes are still fast and sequential? By using an LSM-Tree, i.e., add records to an in-memory balanced tree and then flush to disk in the SST format

## Persisting Trees on Disk

- How might you take a BST and persist it on disk?
  - It kinda feels like B-Trees are a lower level of abstraction because we're working directly with blocks here
  - I'm personally having a difficult time picturing disk pointers—are they just byte offsets?
- This requires a detour to how hard drives work imo

### How do Hard Drives Work?
