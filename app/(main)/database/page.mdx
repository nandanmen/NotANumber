import { HardDrive } from "./_components/hard-drive";
import { FileResizing } from "./_components/file-resizing";
import {
  FileDatabaseVisualizer,
  FileDatabaseControls,
} from "./_components/mutable-database";
import { createStore } from "./_lib/file-database";
import { FileSequence } from "./_components/file-sequence";
import { Compaction } from "./_components/compaction";
import { BuildIndex } from "./_components/build-index";
import { Algorithm } from "./_components/algorithm";

If you were to build your own database today, not knowing that databases exist already, how would you do it? In this post, we'll explore how to build a **key-value database** from the ground up.

A key-value database works more or less like objects in JavaScript—you can store values using a key and retrieve them later using that same key:

```sh
$ db set 'hello' 'world'
$ db get 'hello'
world
```

Let's find out how they work!

---

<ScrollGroup 
  figure={<FileDatabaseVisualizer />}
  state={createStore(
    [
      { type: "set", key: 1, value: "Lorem ipsum" },
      { type: "set", key: 18, value: "dolor sit" },
    ],
    { mutable: true },
  )}
>

<ScrollGroupSection>

## The Humble File

Databases were made to solve one problem:

<ProblemStatement>

How do we store data **persistently** and then **efficiently** look it up later?

</ProblemStatement>

The typical way to store any kind of data persistently in a computer is to use a <Annotation type="box"> _file_ </Annotation>. When we want to store data, we add the key-value pair to the file:

<FileDatabaseControls mode={["add"]} />

When we want to look for a specific key, we iterate through the pairs to see if there's a matching key:

<FileDatabaseControls mode={["search"]} />

For updates, we'll find the key and replace the value in-place:

<FileDatabaseControls mode={["update"]} />

And for deletes, we'll delete the record from the file:

<FileDatabaseControls mode={["delete"]} />

Easy! We're done right?

</ScrollGroupSection>

</ScrollGroup>

---

### Mutable Updates

This approach, simple as it is, doesn't actually work very well in practice. The problem lies with the way we're doing updates and deletes—they're wholly inefficient.

To a computer, our file looks something like this—nothing more than a long sequence of bytes:

<FileSequence />

When we go to update or delete a record, we're currently updating that record in-place, which means we potentially have to _move_ all of the data that comes after that record:

<FileSequence updateable />

In this case, updating the record `005` to "`adipiscing␣elit.␣vel␣mauris`" means moving all of the records that come after it by 11 bytes (the length of the added string "`␣vel␣mauris`"). This can quickly get really costly, especially as our database grows in size!

---

<ScrollGroup
  figure={<FileDatabaseVisualizer />}
  state={createStore([
    { type: "set", key: 1, value: "Lorem ipsum" },
    { type: "set", key: 18, value: "dolor sit" },
  ])}
>

<ScrollGroupSection>

### Append-Only Files

One way to work around the update problem is to **make records immutable**. In other words, we add the constraint that we can only _add_ new records to the end of the file and never update or delete existing ones.

With this approach, updates are treated the same as inserts—just add a new record to the end of the file:

<FileDatabaseControls mode={["add", "update"]} />

But now we have another problem—there are duplicate keys in the file!

To work around this, we have to change our search algorithm to look for the _last_ occurrence of the key instead of the first:

<FileDatabaseControls mode={["search"]} />

To delete records, we create a special "tombstone" record that marks the key as deleted. There's no single way to do this, but one way is to use a special value like `null`:

<FileDatabaseControls mode={["add", "delete"]} />

And there we have it! We have a key-value database that uses a file as its storage mechanism. Using it, we can store, find, update, and delete key-value pairs.

</ScrollGroupSection>

</ScrollGroup>

---

Now this implementation isn't perfect; right now, there are two major issues:

1. **The file can get very large**. Since we're only appending to the file, the file will grow infinitely over time. Not good!
2. **Searching is slow**. To search for a specific key, we have to potentially iterate through all records in the database. For a database with millions of records, this can take a while!

How can we fix these problems?

---

<ScrollGroup figure={<FileDatabaseVisualizer />} state={createStore([
    { type: "set", key: 1, value: "Lorem ipsum" },
    { type: "set", key: 18, value: "dolor sit" },
    { type: "set", key: 7, value: "adipiscing elit." },
    { type: "delete", key: 7 },
    { type: "set", key: 10, value: "consectetur adipiscing elit." },
    { type: "delete", key: 1 },
  ])}>

<ScrollGroupSection>

## Keeping Files Small

<ProblemStatement>

**How do we make sure the file doesn't grow indefinitely?** Because we're using an append-only file, we need some mechanism to periodically "shrink" the file so it doesn't eventually take over our entire hard drive.

</ProblemStatement>

Take a look at our database here after a few updates and deletes:

<FileDatabaseControls mode={[]} />

Our database file has six entries, but only two represent actual records—the rest are either deleted or contain stale data. If we can clear all the irrelevant data, we can shrink the file by over 66%!

</ScrollGroupSection>

</ScrollGroup>

---

### Segments and Compaction

<InlineNote>

Here's an idea: once a file exceeds a certain size, we'll <Annotation type="underline"> _close_ </Annotation> the file and create a new one. While the new file ingests new data (in the same way we've been doing so far), we'll _compact_ the old file by deleting all of its irrelevant data.

<Note>

Meaning, we stop adding new data to the file.

</Note>

</InlineNote>

Here, we've set the maximum file size to seven records. Notice that the database is full—try clicking on "Add" to add a new record and notice what happens:

<Compaction />

Now, our database consists of two different files; we'll call these files **segments**.

Notice that the segment becomes a lot smaller after compaction, which means if we have multiple segment files, we can potentially merge them together as part of the compaction process.

---

### Adapting to Segments

<InlineNote>

Since our database is now partitioned across multiple segments, we'll need to change our search algorithm to search across all of the segments, starting with the <Annotation type="underline"> _newest_ </Annotation> segment first:

<Note>

The newest segment will always contain the most up-to-date data.

</Note>

</InlineNote>

```
segment search example
```

With that, we've made a mechanism to stop our database from growing indefinitely! That's one problem solved.

---

## Your First Index

Our next problem is on search performance:

<ProblemStatement>

**How do we make searching fast?** Right now, we have to iterate through all of the records in the database to find a specific key. This is super slow!

</ProblemStatement>

What if we use _objects_? That's right, these little guys:

```js
const hashTable = {};
```

In other languages, JavaScript objects are called _hash tables_ or _dictionaries_, and they're really good at storing and looking up key-value pairs:

```js
const hashTable = {
  "hello": "world",
  "foo": "bar",
  "baz": "qux",
}
const value = hashTable["hello"]; // "world"
```

Regardless of how many records you have, the time it takes to look up a value in a hash table is more or less constant. The catch is they must live _in memory_.

---

<ScrollGroup>

<ScrollGroupSection>

### Aside: How Computers Store Data

What does it mean for something to be "in-memory"? If a data structure that can store and retrieve key-value pairs already exists, why do we still need to design a database? To answer these questions, we'll need to briefly take a look at how computers store data.

Computers store data in one of two places: **volatile** or **non-volatile** memory. When you write variables in your code, they're stored (for the most part) in your computer's volatile memory, typically [RAM](https://en.wikipedia.org/wiki/Random-access_memory):

```js
const x = 1;
```

Volatile memory is really good for <Annotation type="underline"> _fast, temporary_ </Annotation> storage. Temporary is the keyword here—when the program is stopped or the computer shuts down, all of the data in RAM related to the program is lost. The tradeoff here is that RAM is very fast, with high-end RAM devices reaching speeds of around 56 GB/s.

</ScrollGroupSection>

<ScrollGroupSection>

Non-volatile memory, like [hard disk drives (HDDs)](https://en.wikipedia.org/wiki/Hard_disk_drive), [solid state drives (SSDs)](https://en.wikipedia.org/wiki/Solid-state_drive), or [NVMe drives](https://en.wikipedia.org/wiki/NVM_Express) are really good when you need your data to <Annotation type="underline"> _persist_ </Annotation> instead.

When you write data to a file, that data will be stored in your computer's non-volatile memory:

```js
fs.writeFileSync("data.txt", "Hello, world!");
```

Unlike volatile memory, non-volatile memory will stick around even if the program is stopped or the computer is turned off. The catch is that it's much slower than volatile memory:

- Hard drives: 100 MB/s (500x slower than RAM)
- SSDs: 500 MB/s (100x slower than RAM)
- NVMe drives: 6 GB/s (9x slower than RAM)

It's perfectly possible to create a database using only variables in code and in-memory data structures like a hash table:

```js
const db = {};

const add = (key, value) => {
  db[key] = value;
};

const get = (key) => {
  return db[key];
};
```

But this database would be a pretty bad one—whenever the program crashes or the computer restarts, all the data would be lost! For that reason, we'll need to use **non-volatile** memory to store our data.

</ScrollGroupSection>

</ScrollGroup>

---

Here's how the index will work. For every record that we have in our database, we'll store that record's **offset**—the number of bytes from the beginning of the file to the start of the record—in the index:

<BuildIndex />

The second record, `18: dolor sit`, for example, has an offset of 15 because:
eedw
1. Each character is 1 byte large;
2. The first record is 13 characters long (`1:Lorem ipsum`);
3. The first record ends with a newline character, which is (at most) 2 bytes long;

This gives us an offset of `13 + 2 = 15`.

One thing to note is that we need an index for each segment because the offset is relative to the start of the file—in other words, the start of each segment.

---

<ScrollGroup>

<ScrollGroupSection>

### Searching With Indices

Using an index, our search algorithm can now run a lot more efficiently:

1. Starting at the most recent segment, look up the key in the index;
2. If the key is found, read the record at the offset;
3. If the key is not found, move on to the next segment;
4. Repeat (2) and (3) until the key is found or all segments have been searched.

```
searching with index
```

</ScrollGroupSection>

<ScrollGroupSection>

### Updating Indices

Of course, we'll need to keep our indices up to date with the data or they won't be useful otherwise. Whenever we update, delete, or insert a record, we have to change the index accordingly:

```
updating index
```

Notice what this implies—**writing to the database is slower with an index!** This is one of the tradeoffs of using an index; we can search for data much faster at the cost of slower writes.

</ScrollGroupSection>

</ScrollGroup>

---

### Tradeoffs

An index is great because it lets us query our database much faster, but there are some problems with our specific hash table implementation:

1. **Keys have to fit in memory**. Since we're using an in-memory hash table as our index, all of the keys in our database must fit in memory. This means there's a limit on the number of keys we can store!
2. **Range queries are inefficient**. Our index wouldn't help for search queries; if we wanted to find all the records between the keys `12` and `18`, for example, we'd have to iterate through the entire database!

---

## Sorted String Tables

Here's an idea: what if we **ensure our database is always sorted by key?** By sorting our data, we can immediately make range queries fast:

<Algorithm />

Other than making range queries efficient, SSTs are also great for **reducing the memory footprint** of our index. Since our data is now sorted, we no longer need to store the offset of _every_ record in memory—we can store a much smaller number of keys and still keep most of the benefits of an index:

```
sparse index
```

---

Ensuring our database is always sorted is much easier said than done; by definition, sorting data requires moving around records as new ones get added which is super slow when we're storing data on-disk. This brings us to our problem:

<ProblemStatement>

**How do we keep our data sorted _and_ append-only?** It's too slow to sort the data on-disk every time we add a new record; is there another way?

</ProblemStatement>

---

<ScrollGroup>

<ScrollGroupSection>

The trick is to first **sort the data in memory**, and _then_ write it to disk.

1. When we add a new record, add it to a sorted in-memory list;
2. When our in-memory list gets too large, we'll write it to disk;
3. When we want to read a record, we'll read the in-memory list first, and then the disk if necessary.

The data structure used to store the in-memory list is usually one optimized for sorted data like a **balanced binary search tree**.

</ScrollGroupSection>

<ScrollGroupSection>

Of course, the main downside of having some of your data in-memory is that it's not persistent—if the program crashes or the computer shuts down, all of the data in the in-memory list is lost.

The fix here is thankfully pretty straightforward—every time we add a record to the list, **we also write it to an append-only file on disk**. This way, we have a backup in case a crash does happen (which it most certainly will).

</ScrollGroupSection>

</ScrollGroup>

---

<ScrollGroup>

<ScrollGroupSection>

<SkipLink label="I know how balanced BSTs work" to="#lsm-trees">

### Aside: Balanced Binary Search Trees

</SkipLink>

In a binary search tree, each key is stored in what's called a _node_. Each node has at most two children, with the left child being a value _smaller_ than the node's key and the right child being a value _greater_ than the node's key.

```
add records
```

</ScrollGroupSection>

<ScrollGroupSection>

Organizing the data this way makes searching for data really fast. Since about half the data is either smaller or greater than the key we're looking for, we can immediately narrow down our search to one of the two children:

```
search records
```

</ScrollGroupSection>

<ScrollGroupSection>

The catch is the tree must be balanced, i.e. half the data is on the left side of the tree and half is on the right. If it's not, then the search isn't any faster than looking through all of the items in the tree:

```
slow search example
```

</ScrollGroupSection>

<ScrollGroupSection>

To keep the tree balanced, we'll need to rebalance it whenever we add or remove data from the tree. Try adding a few records here and notice that the tree rotates every so often to keep itself balanced:

```
rebalancing tree
```

</ScrollGroupSection>

</ScrollGroup>

---

### LSM Trees

This combination of an in-memory list (often called a _memtable_) and an on-disk file (typically called a _sorted string table_ or SST) is called an LSM or Log-Structured Merge Tree. LSM trees are the underlying data structure used for large-scale key-value databases like [Google's LevelDB](https://github.com/google/leveldb) and [Amazon's DynamoDB](https://aws.amazon.com/dynamodb/).

LSM trees are great and are proven to perform well at scale (if DynamoDB's peak [80 million requests per second](https://aws.amazon.com/blogs/aws/amazon-prime-day-2020-powered-by-aws/) is any indication). Yet despite that, they still have their own drawbacks.

While LSM trees boast very fast writes, they come at the cost of slower (albeit still fast) reads, since each read requires going through possibly all of the indices in the database to check if a key exists:

```
lsm read example
```

If our application tends to have a lot more reads than writes, than LSM trees may not necessarily be the best fit.

---

## B-Trees

If LSM trees aren't that great for reads, what other options do we have? Enter the **B-Tree**. B-Trees work by taking the binary search tree that we saw earlier and making them work _on disk_.

---

<ScrollGroup>

<ScrollGroupSection>

<SkipLink label="I know how various storage devices work" to="#b-trees">

### Aside: Persistent Storage

</SkipLink>

Volatile memory like RAM are **byte-addressable** which means 

Hard disk drives work by storing data on a series of _spinning disks_. Each disk is divided into _tracks_, which are further divided into _sectors_.

**A sector is the smallest unit of data that can be read or written to a hard drive.** If a sector is 4KB (4 kilobytes, or 4096 bytes) large, you can only read or write data in 4KB chunks—it’s not possible to update individual bytes like you can with RAM.

To read or write sectors, a _disk head_ has to physically move over the sector. This involves positioning the disk head over the correct track (a process known as seeking) and waiting for the sector to move below it.

Seeking is by and large the main contributor to latency in a hard drive. This means if we want our database to be fast, we need to **minimize the number of times we need to seek**.

</ScrollGroupSection>

</ScrollGroup>

---

### Your access patterns matter

Reading data _sequentially_ on a hard drive is much faster than reading data _randomly_.

{/* <Callout label="Why is appending to a file faster?"> */}

A disk in a hard drive is divided into _tracks_, with each track divided into _sectors_. Each sector is a fixed size, and the hard drive can only read or write data in whole sectors using a _disk head_.

When the hard disk reads or writes data, it needs to move the disk head to the specific sector. Moving the disk head is much slower than simply reading or writing data, so we want to minimize the number of times the disk head is moved.

When we append to a file, we're always adding data to the end of the file, so we only need to move the disk head once. When we modify a file, we potentially have to move the disk head multiple times, since we might need to read or write data from different sectors. This makes modifying a file much slower!

{/* </Callout> */}
